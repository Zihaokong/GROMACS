==> Found no new compilers
==> Compilers are defined in the following files:
    /shared/home/ccuser/.spack/linux/compilers.yaml
                      :-) GROMACS - gmx mdrun, 2020.4 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf      Artem Zhmurov   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2020.4
Executable:   /shared/home/ccuser/gromacs2020/bin/gmx_mpi
Data prefix:  /shared/home/ccuser/gromacs2020
Working dir:  /shared/home/ccuser/GROMACS/task2_run/OPLS_run2_sim
Command line:
  gmx_mpi mdrun -s production.tpr -g md_replica_OPLS_1.log -o md_replica_OPLS_1.trr -e md_replica_OPLS_1.edr

Reading file production.tpr, VERSION 2020.4 (single precision)
Changing nstlist from 20 to 80, rlist from 1.226 to 1.332

Using 180 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads per MPI process


WARNING: This run will generate roughly 35509 Mb of data

starting mdrun 'VIRAL PROTEIN in water'
2000000 steps,   4000.0 ps.

Writing final coordinates.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 20.7%.
 The balanceable part of the MD step is 58%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 12.0%.
 Average PME mesh/force load: 1.269
 Part of the total run time spent waiting due to PP/PME imbalance: 16.3 %

NOTE: 12.0 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You might want to use dynamic load balancing (option -dlb.)
      You can also consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 16.3 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


NOTE: 5 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:  4297233.810     5968.579    71997.6
                         1h39:28
                 (ns/day)    (hour/ns)
Performance:       57.903        0.414

GROMACS reminds you: "Creativity in science, as in art, cannot be organized. It arises spontaneously from individual talent. Well-run laboratories can foster it, but hierarchical organizations, inflexible bureaucratic rules, and mountains of futile paperwork can kill it." (Max Perutz)


real	99m36.076s
user	0m0.016s
sys	0m0.031s
                      :-) GROMACS - gmx mdrun, 2020.4 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf      Artem Zhmurov   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2020.4
Executable:   /shared/home/ccuser/gromacs2020/bin/gmx_mpi
Data prefix:  /shared/home/ccuser/gromacs2020
Working dir:  /shared/home/ccuser/GROMACS/task2_run/OPLS_run2_sim
Command line:
  gmx_mpi mdrun -s production.tpr -g md_replica_OPLS_2.log -o md_replica_OPLS_2.trr -e md_replica_OPLS_2.edr

Reading file production.tpr, VERSION 2020.4 (single precision)
Changing nstlist from 20 to 80, rlist from 1.226 to 1.332

Using 180 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads per MPI process


WARNING: This run will generate roughly 35509 Mb of data

starting mdrun 'VIRAL PROTEIN in water'
2000000 steps,   4000.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 17.3%.
 The balanceable part of the MD step is 59%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 10.2%.
 Average PME mesh/force load: 1.298
 Part of the total run time spent waiting due to PP/PME imbalance: 17.9 %

NOTE: 10.2 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You might want to use dynamic load balancing (option -dlb.)
      You can also consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 17.9 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


NOTE: 5 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:  4295899.605     5966.771    71997.1
                         1h39:26
                 (ns/day)    (hour/ns)
Performance:       57.921        0.414

GROMACS reminds you: "He's using code that only you and I know" (Kate Bush)


real	99m31.922s
user	0m0.018s
sys	0m0.035s
